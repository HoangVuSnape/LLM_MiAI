{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwtXZX-tLYqt"
      },
      "source": [
        "# **SimpleChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-VfUu2sTN1cp"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain_community.llms import CTransformers\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8lJAf0bLdov"
      },
      "source": [
        "# **Embedding and Store vector DB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IbgRygUC5i5U"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "teqTPNRI6gLM"
      },
      "outputs": [],
      "source": [
        "# Khai bao bien\n",
        "data_path = \"data\"\n",
        "vector_db_path = \"vectorstores\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jpOT8DdAFY98"
      },
      "outputs": [],
      "source": [
        "def create_db_from_text():\n",
        "  text = \"\"\"LLMs have become a household name thanks to the role they have played\n",
        "  in bringing generative AI to the forefront of the public interest, as well as\n",
        "  the point on which organizations are focusing to adopt artificial intelligence\n",
        "  across numerous business functions and use cases.\n",
        "  Outside of the enterprise context, it may seem like LLMs have arrived out of the\n",
        "  blue along with new developments in generative AI. However, many companies, including\n",
        "  IBM, have spent years implementing LLMs at different levels to enhance their natural\n",
        "  language understanding (NLU) and natural language processing (NLP) capabilities.\n",
        "  This has occurred alongside advances in machine learning, machine learning models,\n",
        "  algorithms, neural networks and the transformer models that provide the architecture for these AI systems.\n",
        "  LLMs are a class of foundation models, which are trained on enormous amounts of\n",
        "  data to provide the foundational capabilities needed to drive multiple use cases\n",
        "  and applications, as well as resolve a multitude of tasks. This is in stark contrast\n",
        "  to the idea of building and training domain specific models for each of these use\n",
        "  cases individually, which is prohibitive under many criteria (most importantly cost\n",
        "  and infrastructure), stifles synergies and can even lead to inferior performance.\"\"\"\n",
        "\n",
        "  splitter = CharacterTextSplitter(chunk_size=500,\n",
        "                                   chunk_overlap=50,\n",
        "                                   )\n",
        "  chunks = splitter.split_text(text)\n",
        "\n",
        "  #Embedding\n",
        "  embedd_model = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "  gpt4all_kwargs = {'allow_download': 'True'}\n",
        "\n",
        "  embeddings = GPT4AllEmbeddings(\n",
        "    model_name=embedd_model,\n",
        "    gpt4all_kwargs=gpt4all_kwargs\n",
        "  )\n",
        "\n",
        "  #Store vector DB into FAISS\n",
        "  db = FAISS.from_texts(chunks, embeddings)\n",
        "  db.save_local(vector_db_path)\n",
        "\n",
        "  return db\n",
        "\n",
        "def create_db_from_files(data_path):\n",
        "  loader = DirectoryLoader(data_path, glob=\"**/*.pdf\", loader_cls = PyPDFLoader)\n",
        "  documents = loader.load()\n",
        "\n",
        "  splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
        "  documents = splitter.split_documents(documents)\n",
        "\n",
        "  #Embeddings\n",
        "  embedd_model = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "  gpt4all_kwargs = {'allow_download': 'True'}\n",
        "\n",
        "  embeddings = GPT4AllEmbeddings(\n",
        "    model_name=embedd_model,\n",
        "    gpt4all_kwargs=gpt4all_kwargs\n",
        "  )\n",
        "  db = FAISS.from_documents(documents, embeddings)\n",
        "  db.save_local(vector_db_path)\n",
        "\n",
        "  return db\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwByJLrXHRPC",
        "outputId": "d402f3be-28fe-4733-ae76-c1d71d69a2b4"
      },
      "outputs": [],
      "source": [
        "create_db_from_files(data_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEsIN1zTLmzR"
      },
      "source": [
        "# **Retriver**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cDhv5TY4K_rN"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import CTransformers\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q6q0dDviLBwG"
      },
      "outputs": [],
      "source": [
        "def load_llm(model_name):\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    llm = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "# Create QA chain\n",
        "def create_qa_chain(prompt, llm, db):\n",
        "    llm_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=db.as_retriever(search_kwargs={\"k\": 3}, max_tokens_limit=1024),\n",
        "        return_source_documents=False,\n",
        "        chain_type_kwargs={'prompt': prompt}\n",
        "    )\n",
        "    return llm_chain\n",
        "\n",
        "# Load vector DB\n",
        "def load_vector_db(vector_db_path):\n",
        "    # Embedding\n",
        "    model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "    gpt4all_kwargs = {'allow_download': 'True'}\n",
        "\n",
        "    embeddings = GPT4AllEmbeddings(\n",
        "      model_name=model_name,\n",
        "      gpt4all_kwargs=gpt4all_kwargs\n",
        "    )\n",
        "\n",
        "    # Allow dangerous deserialization\n",
        "    db = FAISS.load_local(vector_db_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
        "    return db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "kE_WbotCMAmi",
        "outputId": "23cca7ca-9ea4-4799-b6b9-937600112e60"
      },
      "outputs": [],
      "source": [
        "# Cau hinh\n",
        "model_file = \"uonlp/Vistral-7B-Chat-gguf\"\n",
        "vector_db_path = \"./vectorstores\"\n",
        "\n",
        "db = load_vector_db(vector_db_path)\n",
        "llm = load_llm(model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG5ocx_YNYQV"
      },
      "outputs": [],
      "source": [
        "template = '''<s>[INST] <<SYS>>\n",
        "Bạn là một trợ lí Tiếng Việt nhiệt tình và trung thực. Hãy luôn trả lời một cách hữu ích nhất có thể, đồng thời giữ an toàn.\n",
        "Câu trả lời của bạn không nên chứa bất kỳ nội dung gây hại, phân biệt chủng tộc, phân biệt giới tính, độc hại, nguy hiểm hoặc bất hợp pháp nào. Hãy đảm bảo rằng các câu trả lời của bạn không có thiên kiến xã hội và mang tính tích cực.Nếu một câu hỏi không có ý nghĩa hoặc không hợp lý về mặt thông tin, hãy giải thích tại sao thay vì trả lời một điều gì đó không chính xác. Nếu bạn không biết câu trả lời cho một câu hỏi, hãy trẳ lời là bạn không biết và vui lòng không chia sẻ thông tin sai lệch.\n",
        "<</SYS>>\n",
        "\n",
        "{prompt} [/INST]'''\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p57c6HbCboU_"
      },
      "outputs": [],
      "source": [
        "chain_type_kwargs = {\"prompt\": prompt}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7wbtJ21cFlX"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5nbuqencKaW"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_input = \"Hãy cho tôi biết về thủ đô của Việt Nam.\"\n",
        " # Format the prompt using the template\n",
        "formatted_prompt = prompt.format(prompt=user_input)\n",
        "\n",
        "qa_chain = create_qa_chain(formatted_prompt, llm, db)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa0CGGjcp1f8"
      },
      "outputs": [],
      "source": [
        "# Example usage of the QA chain\n",
        "context = \"I am a senior student at TDT University, I am majoring in Computer Science\"\n",
        "question = \"Are you a student?\"\n",
        "\n",
        "response = qa_chain({\"context\": context, \"query\": question})\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
